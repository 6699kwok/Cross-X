"""
CrossX ç”Ÿäº§çº§ RAG æ•°æ®æ¸…æ´—ä¸æ‰¹é‡å…¥åº“æµæ°´çº¿ (Production Pipeline)
============================================================
ç”¨æ³•:
    pip install langchain langchain-openai pymilvus python-frontmatter tiktoken
    export OPENAI_API_KEY="sk-..."
    export MILVUS_URI="http://localhost:19530"   # æˆ–è…¾è®¯äº‘ VectorDB åœ°å€
    python rag_ingest_pipeline.py

æœ¬è„šæœ¬ç”¨äºå°† lib/knowledge/docs/ ä¸‹çš„æ‰€æœ‰ Markdown çŸ¥è¯†åº“æ–‡æ¡£
æ¸…æ´—ã€åˆ‡ç‰‡åå†™å…¥ Milvus å‘é‡æ•°æ®åº“ã€‚

å…³é”®ç‰¹æ€§:
  - ä¸¥æ ¼ YAML å…ƒæ•°æ®æ ¡éªŒï¼ˆæ‹¦æˆªè„æ•°æ®å…¥åº“ï¼‰
  - åŸºäº Markdown æ ‡é¢˜å±‚çº§çš„è¯­ä¹‰åˆ‡ç‰‡ï¼ˆä¿è¯æ¯ä¸ª Q&A å®Œæ•´å…¥åº“ï¼‰
  - å­—ç¬¦çº§å…œåº•åˆ‡ç‰‡ï¼ˆé˜²æ­¢è¶…é•¿ç­”æ¡ˆæˆªæ–­ï¼‰
  - RBAC å…ƒæ•°æ®ç»‘å®šï¼ˆaudience: b2c/b2b éš chunk å­˜å…¥å‘é‡åº“ï¼‰
  - å®Œæ•´çš„æ—¥å¿—è®°å½•ï¼ˆç”Ÿäº§ Ops å¿…å¤‡ï¼‰
"""

import os
import glob
import logging
import frontmatter
from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Milvus
from langchain.schema import Document

# ---------------------------------------------------------------------------
# 1. é…ç½®æ—¥å¿—è®°å½• (Ops å¿…å¤‡)
# ---------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("rag_ingestion.log", encoding="utf-8"),
        logging.StreamHandler(),
    ],
)

# ---------------------------------------------------------------------------
# 2. æ ¸å¿ƒé…ç½®
# ---------------------------------------------------------------------------
DATA_DIR = os.path.join(os.path.dirname(__file__), "../lib/knowledge/docs")
MILVUS_URI = os.environ.get("MILVUS_URI", "http://localhost:19530")
COLLECTION_NAME = "crossx_knowledge_base"
EMBEDDING_MODEL = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_key=os.environ.get("OPENAI_API_KEY"),
)

# Metadata schema â€” all required fields for RBAC and filtering
REQUIRED_METADATA_KEYS = ["doc_id", "category", "target_country", "audience"]
VALID_AUDIENCES = {"b2c", "b2b"}
VALID_CATEGORIES = {"visa", "payment", "transport", "culture", "b2b_protocol", "lifestyle", "accommodation", "emergency"}


# ---------------------------------------------------------------------------
# 3. ä¸¥æ ¼çš„æ•°æ®æ¸…æ´—ä¸æ ¡éªŒè§„åˆ™ (Data Validation)
# ---------------------------------------------------------------------------
def validate_metadata(metadata: dict, file_path: str) -> bool:
    """
    æ ¡éªŒ YAML å¤´ä¿¡æ¯æ˜¯å¦ç¬¦åˆ CrossX é»„é‡‘æ ‡å‡†ã€‚
    ä¸ç¬¦åˆè§„èŒƒçš„æ–‡ä»¶ç›´æ¥æ‹¦æˆªï¼Œä¸å…¥åº“ã€‚
    """
    # æ£€æŸ¥å¿…å¡«é¡¹
    for key in REQUIRED_METADATA_KEYS:
        if key not in metadata:
            logging.error(f"[æ‹¦æˆª] æ–‡ä»¶ {file_path} ç¼ºå¤±å…³é”®æ ‡ç­¾: '{key}'")
            return False

    # æ ¡éªŒ audience æšä¸¾å€¼
    audience = str(metadata.get("audience", "")).lower().strip()
    if audience not in VALID_AUDIENCES:
        logging.error(
            f"[æ‹¦æˆª] æ–‡ä»¶ {file_path} audience å€¼ä¸åˆæ³•: '{audience}'ã€‚"
            f"åˆæ³•å€¼ä¸º: {VALID_AUDIENCES}"
        )
        return False

    # æ ¡éªŒ category æšä¸¾å€¼ (è­¦å‘Šä½†ä¸æ‹¦æˆªï¼Œå…è®¸è‡ªå®šä¹‰ category)
    category = str(metadata.get("category", "")).lower().strip()
    if category not in VALID_CATEGORIES:
        logging.warning(
            f"[è­¦å‘Š] æ–‡ä»¶ {file_path} category å€¼ '{category}' ä¸åœ¨æ ‡å‡†é›†åˆä¸­ã€‚"
            f"æ ‡å‡†é›†åˆ: {VALID_CATEGORIES}"
        )

    # B2B æ–‡æ¡£å¿…é¡»æœ‰ clearance å­—æ®µ
    if audience == "b2b" and "clearance" not in metadata:
        logging.warning(
            f"[è­¦å‘Š] B2B æ–‡ä»¶ {file_path} ç¼ºå°‘ 'clearance' å­—æ®µï¼Œå»ºè®®è¡¥å…… clearance: high"
        )

    return True


# ---------------------------------------------------------------------------
# 4. å•æ–‡ä»¶å¤„ç†ä¸åˆ‡ç‰‡é€»è¾‘
# ---------------------------------------------------------------------------
def process_single_file(file_path: str) -> list:
    """
    å¤„ç†å•ä¸ª Markdown æ–‡ä»¶ï¼š
    1. è§£æ YAML frontmatter å’Œæ­£æ–‡
    2. æ ¡éªŒå…ƒæ•°æ®
    3. åŸºäºæ ‡é¢˜å±‚çº§çš„è¯­ä¹‰åˆ‡ç‰‡
    4. å­—ç¬¦çº§å…œåº•åˆ‡ç‰‡
    5. å°†å…¨å±€å…ƒæ•°æ®æ³¨å…¥æ¯ä¸ªåˆ‡ç‰‡
    è¿”å›: List[Document]
    """
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            post = frontmatter.load(f)

        metadata = dict(post.metadata)
        content = post.content

        # è„æ•°æ®è¿‡æ»¤ï¼šæ­£æ–‡è¿‡çŸ­è§†ä¸ºæ— æ•ˆæ–‡ä»¶
        if len(content.strip()) < 50:
            logging.warning(f"[è·³è¿‡] æ–‡ä»¶ {file_path} æ­£æ–‡è¿‡çŸ­ï¼ˆä¸è¶³50å­—ç¬¦ï¼‰ï¼Œè·³è¿‡ã€‚")
            return []

        # æ‰§è¡Œå…ƒæ•°æ®å¼ºæ ¡éªŒ
        if not validate_metadata(metadata, file_path):
            return []

        # â”€â”€ è¯­ä¹‰çº§ Markdown åˆ‡ç‰‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # è¯†åˆ« ## å’Œ ### æ ‡é¢˜ï¼ŒæŠŠæ¯ä¸ª Q&A æ‰“åŒ…æˆå®Œæ•´çš„ä¸€ä¸ªå‘é‡å—ã€‚
        # è¿™æ˜¯è§£å†³"ç­”æ¡ˆè¢«åˆ‡çƒ‚"é—®é¢˜çš„æ ¸å¿ƒã€‚
        headers_to_split_on = [
            ("#", "Header_1"),
            ("##", "Header_2"),
            ("###", "Question"),  # ### çº§åˆ«ç›´æ¥è¯†åˆ«ä¸ºé—®é¢˜èŠ‚ç‚¹
        ]
        markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on,
            strip_headers=False,  # ä¿ç•™æ ‡é¢˜æ–‡æœ¬åœ¨ chunk é‡Œï¼Œæ–¹ä¾¿ LLM ç†è§£ä¸Šä¸‹æ–‡
        )
        md_header_splits = markdown_splitter.split_text(content)

        # â”€â”€ å­—ç¬¦çº§å…œåº•åˆ‡ç‰‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # é˜²æ­¢æŸä¸ªç­”æ¡ˆå†™å¾—å¤ªé•¿è¶…è¿‡ä¸Šä¸‹æ–‡çª—å£ï¼Œåšä¸€æ¬¡å­—ç¬¦çº§å…œåº•ã€‚
        # ä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹ç¬¦å·ä½œä¸ºä¼˜å…ˆåˆ†å‰²ç‚¹ã€‚
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=100,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""],
        )
        final_splits = text_splitter.split_documents(md_header_splits)

        # â”€â”€ ç»„è£…æœ€ç»ˆçš„ Document å¯¹è±¡å¹¶ç»‘å®šå…¨å±€å…ƒæ•°æ® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        docs_to_insert = []
        for i, split in enumerate(final_splits):
            # åˆå¹¶ Markdown æ ‡é¢˜å…ƒæ•°æ®ï¼ˆå¦‚ Question: xxxï¼‰å’Œå…¨å±€ YAML å…ƒæ•°æ®
            combined_metadata = {**metadata, **split.metadata}
            # æ·»åŠ  chunk_index æ–¹ä¾¿è°ƒè¯•
            combined_metadata["chunk_index"] = i
            combined_metadata["source_file"] = os.path.basename(file_path)

            docs_to_insert.append(
                Document(
                    page_content=split.page_content,
                    metadata=combined_metadata,
                )
            )

        logging.info(
            f"[æˆåŠŸ] æ–‡ä»¶ {os.path.basename(file_path)} æ¸…æ´—å®Œæ¯•ï¼Œ"
            f"audience={metadata.get('audience')}, "
            f"ç”Ÿæˆ {len(docs_to_insert)} ä¸ª Chunkã€‚"
        )
        return docs_to_insert

    except Exception as e:
        logging.error(f"[å¼‚å¸¸] å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
        return []


# ---------------------------------------------------------------------------
# 5. æ‰¹é‡æµæ°´çº¿æ‰§è¡Œ
# ---------------------------------------------------------------------------
def run_ingestion_pipeline(drop_old: bool = False):
    """
    æ‰¹é‡æ‰«æ DATA_DIR ä¸‹æ‰€æœ‰ .md æ–‡ä»¶ï¼Œæ¸…æ´—åå†™å…¥ Milvusã€‚

    å‚æ•°:
        drop_old: True = æ¸…ç©ºæ—§é›†åˆåé‡å»ºï¼ˆç”¨äºå…¨é‡é‡åˆ·ï¼‰
                  False = è¿½åŠ æ¨¡å¼ï¼ˆç”¨äºå¢é‡æ›´æ–°ï¼‰
    """
    logging.info("=" * 60)
    logging.info("=== å¼€å§‹æ‰§è¡Œ CrossX RAG æ•°æ®æ¸…æ´—ä¸å…¥åº“æµæ°´çº¿ ===")
    logging.info(f"=== æ•°æ®ç›®å½•: {DATA_DIR}")
    logging.info(f"=== å‘é‡æ•°æ®åº“: {MILVUS_URI} / {COLLECTION_NAME}")
    logging.info(f"=== æ¨¡å¼: {'å…¨é‡é‡å»º' if drop_old else 'å¢é‡è¿½åŠ '}")
    logging.info("=" * 60)

    # è·å–ç›®å½•ä¸‹æ‰€æœ‰çš„ .md æ–‡ä»¶ï¼ˆé€’å½’ï¼‰
    md_files = glob.glob(os.path.join(DATA_DIR, "**/*.md"), recursive=True)
    if not md_files:
        logging.warning(f"ç›®å½• {DATA_DIR} ä¸‹æœªæ‰¾åˆ°ä»»ä½• Markdown æ–‡ä»¶ã€‚")
        return

    logging.info(f"å…±å‘ç° {len(md_files)} ä¸ª Markdown æ–‡ä»¶ï¼Œå¼€å§‹é€ä¸€å¤„ç†...")

    all_docs_to_insert = []
    skipped_count = 0

    for file_path in sorted(md_files):
        docs = process_single_file(file_path)
        if docs:
            all_docs_to_insert.extend(docs)
        else:
            skipped_count += 1

    if not all_docs_to_insert:
        logging.error("æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®å¯ä»¥å…¥åº“ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æŠ¥é”™ä¿¡æ¯ã€‚")
        return

    logging.info(f"æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæ¯•:")
    logging.info(f"  - æœ‰æ•ˆ Chunk æ•°é‡: {len(all_docs_to_insert)}")
    logging.info(f"  - è·³è¿‡/å¤±è´¥æ–‡ä»¶æ•°: {skipped_count}")
    logging.info(f"å‡†å¤‡å†™å…¥å‘é‡æ•°æ®åº“...")

    # åˆ† B2C / B2B ç»Ÿè®¡
    b2c_chunks = [d for d in all_docs_to_insert if d.metadata.get("audience") == "b2c"]
    b2b_chunks = [d for d in all_docs_to_insert if d.metadata.get("audience") == "b2b"]
    logging.info(f"  - B2C chunks: {len(b2c_chunks)}")
    logging.info(f"  - B2B chunks: {len(b2b_chunks)}")

    # æ‰¹é‡è¿æ¥å‘é‡æ•°æ®åº“å¹¶å†™å…¥
    try:
        Milvus.from_documents(
            documents=all_docs_to_insert,
            embedding=EMBEDDING_MODEL,
            connection_args={"uri": MILVUS_URI},
            collection_name=COLLECTION_NAME,
            drop_old=drop_old,
        )
        logging.info("=" * 60)
        logging.info(f"=== ğŸ‰ æˆåŠŸï¼{len(all_docs_to_insert)} ä¸ª Chunk å·²å†™å…¥å‘é‡æ•°æ®åº“ ===")
        logging.info("=" * 60)
    except Exception as e:
        logging.critical(f"å†™å…¥æ•°æ®åº“å¤±è´¥: {str(e)}", exc_info=True)


# ---------------------------------------------------------------------------
# æ£€ç´¢æ—¶å¿…é¡»åŠ  RBAC è¿‡æ»¤æ¡ä»¶ï¼ˆé˜²æ­¢ B2C ç”¨æˆ·æŸ¥åˆ° B2B æœºå¯†ï¼‰
# ---------------------------------------------------------------------------
def retrieve_with_rbac(vector_db, query: str, audience: str, target_country: str = None, top_k: int = 3):
    """
    å¸¦ RBAC æƒé™éš”ç¦»çš„æ ‡å‡†æ£€ç´¢å‡½æ•°ã€‚

    å…³é”®åŸåˆ™: æ°¸è¿œä¸å…è®¸ b2c ç”¨æˆ·æŸ¥åˆ° b2b å†…å®¹ã€‚
    audience å‚æ•°å¿…é¡»ç”±åç«¯æ ¹æ®ç”¨æˆ·ç™»å½•èº«ä»½å†³å®šï¼Œä¸å¯ç”±ç”¨æˆ·è‡ªå·±ä¼ å…¥ã€‚

    ç¤ºä¾‹ç”¨æ³•:
        results = retrieve_with_rbac(vector_db, "how to use DiDi in China", "b2c", "China")
    """
    # æ„å»º Milvus çš„ Metadata è¿‡æ»¤è¡¨è¾¾å¼
    filter_parts = [f'audience == "{audience}"']
    if target_country:
        filter_parts.append(f'target_country == "{target_country}"')
    filter_expr = " and ".join(filter_parts)

    logging.info(f"[æ£€ç´¢] Query='{query[:50]}...', Filter='{filter_expr}'")

    results = vector_db.similarity_search(
        query=query,
        expr=filter_expr,  # æ ¸å¿ƒï¼šRBAC æƒé™éš”ç¦»å°±åœ¨è¿™é‡Œç”Ÿæ•ˆï¼
        k=top_k,
    )
    return results


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="CrossX RAG æ•°æ®å…¥åº“æµæ°´çº¿")
    parser.add_argument(
        "--drop-old",
        action="store_true",
        help="æ¸…ç©ºæ—§é›†åˆåé‡å»ºï¼ˆå…¨é‡é‡åˆ·ï¼Œç”Ÿäº§ç¯å¢ƒè°¨æ…ä½¿ç”¨ï¼‰",
    )
    args = parser.parse_args()

    run_ingestion_pipeline(drop_old=args.drop_old)
